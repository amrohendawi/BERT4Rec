(venv) jovyan@jupyter-amro-2ehendawi-40fokus-2efraunhofer-2ede:~/BERT4Rec$ python BERT4Rec.py 
15 Sep 10:58    INFO  
General Hyper Parameters:
gpu_id = 0
use_gpu = True
seed = 2020
state = INFO
reproducibility = True
data_path = ./training_data/hm
checkpoint_dir = saved
show_progress = False
save_dataset = False
dataset_save_path = None
save_dataloaders = False
dataloaders_save_path = None
log_wandb = False

Training Hyper Parameters:
epochs = 5
train_batch_size = 150
learner = adam
learning_rate = 0.01
neg_sampling = None
eval_step = 1
stopping_step = 10
clip_grad_norm = None
weight_decay = 0.0
loss_decimal_place = 4

Evaluation Hyper Parameters:
eval_args = {'split': {'RS': [4, 1, 1]}, 'group_by': 'None', 'order': 'TO', 'mode': 'uni50'}
repeatable = True
metrics = ['Recall', 'MRR', 'NDCG', 'Hit', 'Precision', 'MAP']
topk = [12]
valid_metric = MAP@12
valid_metric_bigger = True
eval_batch_size = 150
metric_decimal_place = 4

Dataset Hyper Parameters:
field_separator = 
seq_separator =  
USER_ID_FIELD = customer_id
ITEM_ID_FIELD = article_id
RATING_FIELD = None
TIME_FIELD = t_dat
seq_len = None
LABEL_FIELD = label
threshold = None
NEG_PREFIX = neg_
load_col = {'inter': ['customer_id', 'article_id', 't_dat'], 'user': ['customer_id'], 'item': ['article_id']}
unload_col = None
unused_col = None
additional_feat_suffix = None
rm_dup_inter = None
val_interval = None
filter_inter_by_user_or_item = False
user_inter_num_interval = [0,inf)
item_inter_num_interval = [0,inf)
alias_of_user_id = None
alias_of_item_id = None
alias_of_entity_id = None
alias_of_relation_id = None
preload_weight = None
normalize_field = None
normalize_all = None
ITEM_LIST_LENGTH_FIELD = item_length
LIST_SUFFIX = _list
MAX_ITEM_LIST_LENGTH = 50
POSITION_FIELD = position_id
HEAD_ENTITY_ID_FIELD = head_id
TAIL_ENTITY_ID_FIELD = tail_id
RELATION_ID_FIELD = relation_id
ENTITY_ID_FIELD = entity_id
benchmark_filename = None

Other Hyper Parameters: 
wandb_project = recbole
require_pow = False
n_layers = 2
n_heads = 2
hidden_size = 64
inner_size = 256
hidden_dropout_prob = 0.5
attn_dropout_prob = 0.5
hidden_act = gelu
layer_norm_eps = 1e-12
initializer_range = 0.02
mask_ratio = 0.2
loss_type = CE
MODEL_TYPE = ModelType.SEQUENTIAL
MODEL_INPUT_TYPE = InputType.POINTWISE
eval_type = EvaluatorType.RANKING
device = cuda
train_neg_sample_args = {'strategy': 'none'}
eval_neg_sample_args = {'strategy': 'by', 'by': 50, 'distribution': 'uniform'}


15 Sep 10:58    INFO  hm
The number of users: 1371981
Average actions of users: 4.747946459831407
The number of items: 123789
Average actions of items: 9.662172530965691
The number of inters: 176296
The sparsity of the dataset: 99.99989619642594%
Remain Fields: ['t_dat', 'customer_id', 'article_id']
15 Sep 10:58    INFO  [Training]: train_batch_size = [150] negative sampling: [None]
15 Sep 10:58    INFO  [Evaluation]: eval_batch_size = [150] eval_args: [{'split': {'RS': [4, 1, 1]}, 'group_by': 'None', 'order': 'TO', 'mode': 'uni50'}]
15 Sep 10:58    INFO  BERT4Rec(
  (item_embedding): Embedding(123790, 64, padding_idx=0)
  (position_embedding): Embedding(51, 64)
  (trm_encoder): TransformerEncoder(
    (layer): ModuleList(
      (0): TransformerLayer(
        (multi_head_attention): MultiHeadAttention(
          (query): Linear(in_features=64, out_features=64, bias=True)
          (key): Linear(in_features=64, out_features=64, bias=True)
          (value): Linear(in_features=64, out_features=64, bias=True)
          (softmax): Softmax(dim=-1)
          (attn_dropout): Dropout(p=0.5, inplace=False)
          (dense): Linear(in_features=64, out_features=64, bias=True)
          (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (out_dropout): Dropout(p=0.5, inplace=False)
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=64, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=64, bias=True)
          (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
      (1): TransformerLayer(
        (multi_head_attention): MultiHeadAttention(
          (query): Linear(in_features=64, out_features=64, bias=True)
          (key): Linear(in_features=64, out_features=64, bias=True)
          (value): Linear(in_features=64, out_features=64, bias=True)
          (softmax): Softmax(dim=-1)
          (attn_dropout): Dropout(p=0.5, inplace=False)
          (dense): Linear(in_features=64, out_features=64, bias=True)
          (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (out_dropout): Dropout(p=0.5, inplace=False)
        )
        (feed_forward): FeedForward(
          (dense_1): Linear(in_features=64, out_features=256, bias=True)
          (dense_2): Linear(in_features=256, out_features=64, bias=True)
          (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.5, inplace=False)
        )
      )
    )
  )
  (LayerNorm): LayerNorm((64,), eps=1e-12, elementwise_affine=True)
  (dropout): Dropout(p=0.5, inplace=False)
)
Trainable parameters: 8025920
15 Sep 10:59    INFO  epoch 0 training [time: 63.35s, train loss: 5661.6545]
15 Sep 11:01    INFO  epoch 0 evaluating [time: 86.59s, valid_score: 0.417600]
15 Sep 11:01    INFO  valid result: 
recall@12 : 0.7239    mrr@12 : 0.558    ndcg@12 : 0.5413    hit@12 : 0.9149    precision@12 : 0.1552    map@12 : 0.4176
15 Sep 11:01    INFO  Saving current: saved/BERT4Rec-Sep-15-2022_10-58-39.pth
15 Sep 11:02    INFO  epoch 1 training [time: 62.82s, train loss: 5408.3295]
15 Sep 11:03    INFO  epoch 1 evaluating [time: 80.78s, valid_score: 0.423700]
15 Sep 11:03    INFO  valid result: 
recall@12 : 0.7331    mrr@12 : 0.5642    ndcg@12 : 0.5479    hit@12 : 0.9192    precision@12 : 0.1566    map@12 : 0.4237
15 Sep 11:03    INFO  Saving current: saved/BERT4Rec-Sep-15-2022_10-58-39.pth
15 Sep 11:04    INFO  epoch 2 training [time: 64.83s, train loss: 5380.8623]
15 Sep 11:05    INFO  epoch 2 evaluating [time: 80.53s, valid_score: 0.420700]
15 Sep 11:05    INFO  valid result: 
recall@12 : 0.7353    mrr@12 : 0.5584    ndcg@12 : 0.5458    hit@12 : 0.9215    precision@12 : 0.1563    map@12 : 0.4207
15 Sep 11:07    INFO  epoch 3 training [time: 65.18s, train loss: 5365.4222]
15 Sep 11:08    INFO  epoch 3 evaluating [time: 85.29s, valid_score: 0.424400]
15 Sep 11:08    INFO  valid result: 
recall@12 : 0.7401    mrr@12 : 0.564    ndcg@12 : 0.5503    hit@12 : 0.9258    precision@12 : 0.1576    map@12 : 0.4244
15 Sep 11:08    INFO  Saving current: saved/BERT4Rec-Sep-15-2022_10-58-39.pth
15 Sep 11:09    INFO  epoch 4 training [time: 56.65s, train loss: 5356.5618]
15 Sep 11:10    INFO  epoch 4 evaluating [time: 83.03s, valid_score: 0.422900]
15 Sep 11:10    INFO  valid result: 
recall@12 : 0.7386    mrr@12 : 0.5633    ndcg@12 : 0.5488    hit@12 : 0.9228    precision@12 : 0.1571    map@12 : 0.4229
15 Sep 11:10    INFO  Loading model structure and parameters from saved/BERT4Rec-Sep-15-2022_10-58-39.pth
15 Sep 11:12    INFO  best valid : OrderedDict([('recall@12', 0.7401), ('mrr@12', 0.564), ('ndcg@12', 0.5503), ('hit@12', 0.9258), ('precision@12', 0.1576), ('map@12', 0.4244)])
15 Sep 11:12    INFO  test result: OrderedDict([('recall@12', 0.6901), ('mrr@12', 0.5172), ('ndcg@12', 0.5028), ('hit@12', 0.8945), ('precision@12', 0.1413), ('map@12', 0.3805)])